<html>
<head>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
		font-size: 14px;
		width: 15%;
		max-width: 130px;
		position: relative;
		margin-left: 10px;
		text-align: left;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		padding: 5px;
	}
	.margin-right-block {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-size: 14px;
		width: 25%;
		max-width: 256px;
		position: relative;
		text-align: left;
		padding: 10px;
	}

	img {
		max-width: 100%;
		height: auto;
		display: block;
		margin: auto;
	}

	a:link,a:visited {
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	h2 {
		font-size: 16px;
		margin-top: 12px;
		margin-bottom: 8px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: auto;
  }

	table.results {
		border-collapse: collapse;
		margin: 15px 0;
	}

	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: left;
	}

	table.results th {
		background-color: #f0f0f0;
	}

</style>

	  <title>Beyond Local Minima: A Geometric Analysis of the Loss Landscape</title>
      <meta property="og:title" content="Beyond Local Minima: A Geometric Analysis of the Loss Landscape" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Beyond Local Minima: A Geometric Analysis of the Loss Landscape in Overparameterized Neural Networks</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ&list=RDdQw4w9WgXcQ&start_radio=1">Joe Koszut</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ&list=RDdQw4w9WgXcQ&start_radio=1">Theodoros Xenakis</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">December 9, 2025</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
              <a href="#methods">Methods & Experiments</a><br><br>
              <a href="#results">Results & Analysis</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
              <a href="#references">References</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
						<h1>1. Introduction</h1>
            It is widely accepted that overparameterization plays a central role in the remarkable generalization capabilities of modern deep neural networks. Specifically, increasing model capacity leads to loss landscapes characterized by numerous flat valleys or basins, each corresponding to similarly low loss. A central feature of these landscapes is mode connectivity: the phenomenon in which a path of low loss exists between two distinct, well-trained solutions (modes) of the network <a href="#ref_1">[1, 2]</a>. Rather than being isolated points, these minima are linked, making it possible to traverse the loss landscape along a smooth path without encountering significant barriers in loss.<br><br>

            Most existing research has focused on the general existence of such low-loss paths. However, less attention has been paid to the detailed geometric properties, such as sharpness or curvature, of these paths and how they evolve quantitatively with the degree of overparameterization. A quantitative analysis of this geometric evolution is crucial for identifying key factors that govern the loss landscape and generalization performance. This understanding can be instrumental to advancing both the theoretical understanding of deep learning and the practical design of more efficient and robust neural networks. The fundamental nature of this topic makes it broadly applicable across a wide range of deep learning architectures and tasks.
		    </div>
		    <div class="margin-right-block">
						Mode connectivity refers to the existence of low-loss paths connecting different trained solutions in the parameter space of neural networks.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>1.1 Novelty and Significance</h2>
            This project aims to help bridge the gap between properties of the optimization landscape and the practical performance of deep learning models. This will be done by performing a comprehensive geometric analysis of the loss landscape for multiple deep neural networks. The novelty of our work comes from the quantitative investigation of barrier height, path curvature, and sharpness as a function of a single, controlled factor: network depth, a proxy for overparameterization. The results aim to provide intuitive and meaningful insights into how overparameterization influences the landscape and performance of deep learning models. The significance of these results is that they offer a clearer link between loss landscape geometry and empirical performance, enabling more principled design and training strategies of deep learning models.
		    </div>
		    <div class="margin-right-block">
						The study controls for network depth while measuring geometric properties of the loss landscape.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>1.2 Hypotheses</h2>
            The hypotheses for our work include:<br><br>

            <b>- Lower Barrier Heights Accompany Deeper Networks:</b> We hypothesize that increased network depth will lead to lower barrier heights (smaller maximum loss along the path) between connected minima. This suggests that the loss landscape becomes smoother and more connected as overparameterization increases.<br><br>

            <b>- Path Curvature Decreases with Overparameterization:</b> We hypothesize that the curvature of the low-loss paths connecting distinct solutions will decrease as network depth increases, indicating that the mode connectivity approaches a linear relationship.<br><br>

            <b>- Increased Overparameterization Correlates with Flatter Minima:</b> We hypothesize that as the network's depth increases, the local minima found by the optimizer will become flatter. Flatter minima are generally associated with better generalization.<br><br>

            <b>- Path Geometry Directly Predicts Generalization:</b> We hypothesize that a measurable correlation exists between the geometric properties (low barrier height, low curvature, low sharpness) of the found paths and the final test accuracy of the models, thereby providing a quantitative predictor for generalization performance beyond simply training loss.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>2. Background</h1>
					  Empirical and theoretical studies have demonstrated that wider networks tend to exhibit mode connectivity: where distinct minima are connected by a smooth, low-loss path <a href="#ref_1">[1, 2]</a>. These networks tend to occupy regions of the loss landscape that are smoother, with shorter and less pronounced barriers between minima. This characteristic suggests that wider models, by virtue of their increased parameterization, might allow for greater flexibility in optimization, potentially leading to more robust solutions.<br><br>

            However, most prior work has focused on the existence of connectivity, rather than the geometry of these connections. In particular, even fewer studies have measured how landscape properties, such as barrier height and path sharpness, vary not just within one path, but across varying architectural factors like depth in practical deep learning settings on real datasets (e.g., ResNet or CNNs on CIFAR-10).<br><br>

            Moreover, while increased connectivity through overparameterization has been linked to improved generalization, this relationship is not without limits <a href="#ref_3">[3, 4]</a>. Beyond a certain degree of overparameterization, increased capacity may no longer improve generalization. Larger models might exhibit wider but shallower basins in the loss landscape, potentially impairing test performance on out-of-distribution data. Furthermore, increasing depth and width might give opposing results <a href="#ref_5">[5]</a>. The precise conditions under which the benefits of increased connectivity plateau, and how the loss landscape geometry contributes to this effect, remains underexplored.<br><br>

            The collective body of existing research confirms the central role of overparameterization in shaping the loss landscape <a href="#ref_6">[6, 7]</a>. It has been established that the set of optimal solutions in networks where the number of parameters significantly exceeds the number of data points is not discrete but rather forms a high-dimensional submanifold <a href="#ref_8">[8, 9]</a>. Awareness of this phenomenon fundamentally changes the optimization challenge. Finding a good solution requires finding not only low-loss minimum, but also a minimum that generalizes well, a property not reflected in the loss itself. This has motivated work to study the effect of hyperparameters such as learning rate and batch size on the width of the minima that stochastic gradient descent (SGD) converges to <a href="#ref_10">[10]</a>. Furthermore, dedicated flatness-aware optimization strategies, such as sharpness-aware minimization (SAM) <a href="#ref_11">[11]</a> and stochastic weight averaging (SWA) <a href="#ref_12">[12]</a>, have been developed to actively seek out flat minima <a href="#ref_13">[13]</a>.<br><br>

            Furthermore, a substantial area of research has focused on the relationship between optimization and flat minima, hypothesizing that the flatness of a basin correlates directly with a model's superior generalization ability <a href="#ref_14">[14]</a>. However, the connection between these macroscopic landscape properties, like the existence of flat minima, and the microscopic geometry of the connections between them, like path curvature and sharpness, remains loosely quantified, particularly in the context of controlled architectural variations like depth on modern architectures <a href="#ref_6">[6, 7]</a>.
		    </div>
		    <div class="margin-right-block">
						Flatness-aware optimization strategies like sharpness-aware minimization (SAM) and stochastic weight averaging (SWA) have been developed to actively seek out flat minima.
		    </div>
		</div>

		<div class="content-margin-container" id="methods">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>3. Methods & Experiments</h1>
            For each network architecture, we trained two independent models from different random initializations. Each training run produced a set of parameters, denoted A and B, which corresponded to distinct minima of the loss landscape. Using these two solutions as endpoints, we then constructed a low-loss connecting curve in parameter space by introducing and optimizing a third point, C, which defined a nonlinear interpolation between A and B.<br><br>

            This resulted in a parameterized path passing through (A, C, B), along which we sampled and evaluated loss, curvature, and sharpness. For comparison, we also evaluated the straight-line interpolation between A and B, as well as geometric properties of the plane spanned by the three points.<br><br>

            Along each path we evaluated: training and test loss, curvature metrics, maximum barrier height, and generalization gap. This procedure allowed us to compare different architectures not only in terms of whether a low-loss path existed, but also the geometry of it.
		    </div>
		    <div class="margin-right-block">
						The methodology uses Bezier curves and piecewise linear paths to connect independently trained models.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>3.1 Network Architecture</h2>
            To investigate how mode connectivity and loss-landscape geometry varies as a function of architectural complexity, we compared several variants of the same neural network family. Specifically, we evaluated five residual networks of increasing depth: ResNet-8, ResNet-26, ResNet-38, ResNet-65, and ResNet-119.<br><br>

            Residual networks (ResNets) introduce skip connections that allow information to bypass nonlinear layers, enabling efficient optimization of deep architectures and mitigating vanishing gradients <a href="#ref_15">[15]</a>. Beyond their practical advantages, ResNets serve as a useful test case for loss-landscape analysis. Their residual block structure with skip connections and ease of scalability leads to loss landscapes with nontrivial geometry <a href="#ref_9">[9, 16]</a>, while still being compact enough to train extensively under controlled conditions <a href="#ref_1">[1]</a>. By spanning more than an order of magnitude in model size, this architectural sweep allowed us to examine how path geometry, barrier height, and generalization evolve as networks transition from moderately to heavily overparameterized regimes.<br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>Parameters</th>
              </tr>
              <tr>
                <td>ResNet-8</td>
                <td>~80k</td>
              </tr>
              <tr>
                <td>ResNet-26</td>
                <td>~370k</td>
              </tr>
              <tr>
                <td>ResNet-38</td>
                <td>~570k</td>
              </tr>
              <tr>
                <td>ResNet-65</td>
                <td>~660k</td>
              </tr>
              <tr>
                <td>ResNet-119</td>
                <td>~1.2M</td>
              </tr>
            </table>
		    </div>
		    <div class="margin-right-block">
						ResNets were chosen because they outperform plain CNNs on image recognition tasks and have well-studied loss landscapes.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>3.2 Dataset</h2>
            We conducted experiments on two widely used image-classification benchmarks: CIFAR-10 and CIFAR-100. Each dataset consists of 60,000 RGB images of size 32×32, split into 50,000 training and 10,000 test images, covering everyday object categories.<br><br>

            The two datasets differ primarily in granularity: CIFAR-10 has 10 classes with 6,000 images per class, while CIFAR-100 has 100 classes with 600 images per class. CIFAR-100 poses a more challenging classification problem due to the fact that it has fewer images per category. This typically results in lower test accuracy, and will potentially also affect the loss landscape.
		    </div>
		    <div class="margin-right-block">
						These datasets were chosen for their strong baselines and computational feasibility for exhaustive experimentation.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>3.3 Low-Loss Path Search</h2>
            We conducted experiments on two widely used image-classification benchmarks: CIFAR-10 and CIFAR-100. Each dataset consists of 60,000 RGB images of size 32×32, split into 50,000 training and 10,000 test images, covering everyday object categories.<br><br>

            The two datasets differ primarily in granularity: CIFAR-10 has 10 classes with 6,000 images per class, while CIFAR-100 has 100 classes with 600 images per class. CIFAR-100 poses a more challenging classification problem due to the fact that it has fewer images per category. This typically results in lower test accuracy, and will potentially also affect the loss landscape.
		    </div>
		    <div class="margin-right-block">
						These datasets were chosen for their strong baselines and computational feasibility for exhaustive experimentation.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>3.4 Training Configuration</h2>
            We conducted experiments on two widely used image-classification benchmarks: CIFAR-10 and CIFAR-100. Each dataset consists of 60,000 RGB images of size 32×32, split into 50,000 training and 10,000 test images, covering everyday object categories.<br><br>

            The two datasets differ primarily in granularity: CIFAR-10 has 10 classes with 6,000 images per class, while CIFAR-100 has 100 classes with 600 images per class. CIFAR-100 poses a more challenging classification problem due to the fact that it has fewer images per category. This typically results in lower test accuracy, and will potentially also affect the loss landscape.
		    </div>
		    <div class="margin-right-block">
						These datasets were chosen for their strong baselines and computational feasibility for exhaustive experimentation.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>3.5 Compute Hardware</h2>
            We conducted experiments on two widely used image-classification benchmarks: CIFAR-10 and CIFAR-100. Each dataset consists of 60,000 RGB images of size 32×32, split into 50,000 training and 10,000 test images, covering everyday object categories.<br><br>

            The two datasets differ primarily in granularity: CIFAR-10 has 10 classes with 6,000 images per class, while CIFAR-100 has 100 classes with 600 images per class. CIFAR-100 poses a more challenging classification problem due to the fact that it has fewer images per category. This typically results in lower test accuracy, and will potentially also affect the loss landscape.
		    </div>
		    <div class="margin-right-block">
						These datasets were chosen for their strong baselines and computational feasibility for exhaustive experimentation.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>3.6 Metrics</h2>
            We conducted experiments on two widely used image-classification benchmarks: CIFAR-10 and CIFAR-100. Each dataset consists of 60,000 RGB images of size 32×32, split into 50,000 training and 10,000 test images, covering everyday object categories.<br><br>

            The two datasets differ primarily in granularity: CIFAR-10 has 10 classes with 6,000 images per class, while CIFAR-100 has 100 classes with 600 images per class. CIFAR-100 poses a more challenging classification problem due to the fact that it has fewer images per category. This typically results in lower test accuracy, and will potentially also affect the loss landscape.
		    </div>
		    <div class="margin-right-block">
						These datasets were chosen for their strong baselines and computational feasibility for exhaustive experimentation.
		    </div>
		</div>


		
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>Key Metrics</h2>
            To evaluate how network architecture influences the geometry of low-loss paths, we quantify several landscape properties:<br><br>

            <b>Barrier Height:</b> Given a parameterized path between two independently trained models, sampled at N points, we define the barrier height as the difference between the maximum and minimum loss observed along the path. This measures the "cost" of traveling between two minima.<br><br>

            <b>Angle:</b> The relative position of the optimized middle point provides a proxy for how curved the low-loss path must be. We treat the three points (the two minima and the optimized middle point) as a triangle in parameter space and compute the angles of the triangle.<br><br>

            <b>Sharpness:</b> We approximate curvature by estimating the top-k eigenvalues of the Hessian at sampled points along the path. These dominant eigenvalues represent the directions of largest curvature and serve as a proxy for the sharpness of the loss landscape. Large top eigenvalues indicate steep, narrow regions (high sharpness), while smaller values indicate broader, flatter basins.
		    </div>
		    <div class="margin-right-block">
						Computing the full Hessian is intractable for modern neural networks, so we use iterative methods like power iteration.
		    </div>
		</div>

		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>4. Results & Analysis</h1>
            We present a detailed analysis of the loss landscape and connectivity between minima for ResNet models of varying capacity on CIFAR-10 and CIFAR-100 datasets. We primarily focus on Bezier paths, as PolyChain paths exhibited similar trends. The greatest observed differences in results occurs between ResNet-8 and ResNet-26, which correlates with the biggest relative jump in capacity, a ~4.5× increase in parameters.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>4.1 Convex Combination</h2>
            A convex combination of two minima corresponds to a linear interpolation between their parameter vectors. Across all architectures, straight-line interpolation produced similarly large drops in accuracy, indicating that the barrier height along the straight-line path was not reduced by increased model complexity.<br><br>

            However, the width of the low-accuracy region decreased with larger models, suggesting that deeper networks occupied broader basins of good performance, while smaller models traversed extended high-loss regions. Thus, overparameterization did not change the depth of the barrier on the linear path connecting minima, but it did change the area underneath the barrier.<br><br>

            This does not support the hypothesis that barrier height decreases with deeper networks. However, this only applies to linear paths and does not rule out the possibility of the barrier along a curved path decreasing with increased depth.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -50%);">
						Linear paths provide a baseline for understanding loss landscape geometry, but may not reveal the true connectivity between minima.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>4.2 Barrier Height</h2>
            <table class="results">
              <tr>
                <th>Model</th>
                <th>Convex AUC</th>
                <th>Convex Peak</th>
                <th>Bezier AUC</th>
                <th>Bezier Peak</th>
              </tr>
              <tr>
                <td>ResNet-8</td>
                <td>35.0173</td>
                <td>63.74%</td>
                <td>4.1395</td>
                <td>7.02%</td>
              </tr>
              <tr>
                <td>ResNet-26</td>
                <td>39.5827</td>
                <td>85.68%</td>
                <td>2.7302</td>
                <td>5.19%</td>
              </tr>
              <tr>
                <td>ResNet-38</td>
                <td>41.9170</td>
                <td>91.16%</td>
                <td>2.2612</td>
                <td>4.22%</td>
              </tr>
              <tr>
                <td>ResNet-65</td>
                <td>38.1347</td>
                <td>94.06%</td>
                <td>1.1581</td>
                <td>2.29%</td>
              </tr>
              <tr>
                <td>ResNet-119</td>
                <td>34.8687</td>
                <td>96.60%</td>
                <td>1.1634</td>
                <td>2.08%</td>
              </tr>
            </table><br>

            The table shows that smaller networks experienced large dips in accuracy when moving along the low-loss path, whereas larger networks were much more stable. For example, ResNet-8 lost more than 7% test accuracy at its worst point, while ResNet-65 and ResNet-119 dropped only ~2%. This indicates that deeper/wider models are easier to connect smoothly: their loss basins are flatter and more consistent. However, gains seemed to level off beyond ResNet-65, suggesting that very large models may not benefit further in terms of barrier height.
		    </div>
		    <div class="margin-right-block">
						Area Under Curve (AUC) and Peak Height metrics for CIFAR-100 test set.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>4.3 Angle</h2>
            <table class="results">
              <tr>
                <th>Model</th>
                <th>||A-C||₂</th>
                <th>||B-C||₂</th>
                <th>∠ACB (deg)</th>
              </tr>
              <tr>
                <td colspan="4" style="background-color: #f8f8f8;"><b>CIFAR-10</b></td>
              </tr>
              <tr>
                <td>ResNet-8</td>
                <td>38.75</td>
                <td>38.69</td>
                <td>69.39</td>
              </tr>
              <tr>
                <td>ResNet-26</td>
                <td>49.16</td>
                <td>48.49</td>
                <td>62.31</td>
              </tr>
              <tr>
                <td>ResNet-38</td>
                <td>49.78</td>
                <td>53.37</td>
                <td>62.78</td>
              </tr>
              <tr>
                <td>ResNet-65</td>
                <td>50.40</td>
                <td>49.96</td>
                <td>56.91</td>
              </tr>
              <tr>
                <td>ResNet-119</td>
                <td>51.89</td>
                <td>54.48</td>
                <td>55.16</td>
              </tr>
              <tr>
                <td colspan="4" style="background-color: #f8f8f8;"><b>CIFAR-100</b></td>
              </tr>
              <tr>
                <td>ResNet-8</td>
                <td>58.72</td>
                <td>59.81</td>
                <td>70.75</td>
              </tr>
              <tr>
                <td>ResNet-26</td>
                <td>76.25</td>
                <td>78.02</td>
                <td>68.76</td>
              </tr>
              <tr>
                <td>ResNet-38</td>
                <td>85.79</td>
                <td>84.68</td>
                <td>70.18</td>
              </tr>
              <tr>
                <td>ResNet-65</td>
                <td>70.44</td>
                <td>72.95</td>
                <td>65.91</td>
              </tr>
              <tr>
                <td>ResNet-119</td>
                <td>77.92</td>
                <td>78.33</td>
                <td>64.14</td>
              </tr>
            </table><br>

            Since ||A-C|| and ||B-C|| are close to each other, the low-loss path is roughly symmetric, and therefore we can reliably use the interior angle ∠ACB as a proxy for the path's degree of nonlinearity. Smaller angles correspond to a larger outward displacement of C and therefore to a more strongly curved low-loss path.<br><br>

            On CIFAR-10, the angle decreased consistently with model size, from approximately 69° for ResNet-8 to approximately 55° for ResNet-119, indicating that larger models relied on more strongly curved paths to maintain low loss. This goes against our original expectations that the curvature of the low-loss paths would have an inverse correlation with model capacity. A similar, although less smooth, trend was observed on CIFAR-100.
		    </div>
		    <div class="margin-right-block">
						Counter-intuitively, larger networks require more curved paths to connect minima, despite having flatter loss landscapes overall.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>4.4 Sharpness</h2>
            The sharpness of the low-loss path provides insight into the robustness and generalization capabilities of a trained model. Our analysis reveals that, for the same optimizer hyperparameters, smaller models like ResNet-8 tend to find solutions in sharper minima compared to larger models like ResNet-119 which opt instead for flatter, wider basins. This supports our hypothesis.<br><br>

            However, despite the low barrier height for a large model like ResNet-119, our results show that the sharpness still changes by a factor of 2 between the endpoints and the midpoint of the low-loss path. To ensure that this behavior was a more global rather than local property, the average of the five largest eigenvalues of the Hessian was studied, which supported the behavior exhibited by the largest eigenvalue alone.<br><br>

            The sharpness behavior highlights that a low loss alone might not be enough to classify a path as consisting of truly equivalent solutions. This can be relevant in scenarios such as when the low-loss path is being used to create an ensemble of neural networks. If truly similar performance is desired, it can be wise to also consider the sharpness when creating such an ensemble, otherwise, the varying sharpness can lead to unequal generalization capabilities amongst the ensemble.
		    </div>
		    <div class="margin-right-block">
						Sharpness measurements used the top eigenvalues of the Hessian matrix as computed via power iteration.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>4.5 Loss Landscape </h2>
            Visualizations of the test error over a two-dimensional grid in the plane spanned by the two trained solutions and the optimized midpoint revealed how error varies not only along the low-loss path but also in its surrounding region. In all cases, the straight-line interpolation passed through a pronounced high-error ridge, while the optimized curve circumvented this region by bending into areas of lower error.<br><br>

            Notably, the extent and shape of the low-error basins differed systematically across architectures: ResNet-8 exhibited a relatively narrow and fragmented low-error region, with steep transitions into high-error zones, whereas deeper models such as ResNet-65 and ResNet-119 displayed substantially broader low-error basins, within which the optimized path remained confined.<br><br>

            This suggests that larger networks form wider and more coherent regions of good generalization performance, despite requiring a curved path to connect minima. The transition from narrow to broad basins is visible in the progressive flattening and widening of the low-error contours across models, indicating that overparameterization produces landscapes in which low-error regions are not only connected but also spatially expansive.
		    </div>
		    <div class="margin-right-block">
						Two-dimensional slices provide insight into the local structure of the high-dimensional loss landscape.
		    </div>
		</div>
		<img src="./images/test_loss_planes_cifar10.png" width=512px/>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>4.6 Generalization Gap</h2>
            The difference between the train and test accuracies, referred to as the generalization gap, can provide valuable insight into the quality of the solution which goes beyond simply looking at its loss. We hypothesized that increased flatness (induced by increased model capacity) would align with improved generalization capabilities.<br><br>

            For the CIFAR-10 dataset, the generalization gap follows the double descent phenomenon, and the results support our hypothesis that low sharpness (induced by overparameterization) leads to improved generalization along the entire length of the low-loss path. However, the evidence is only mildly supportive. We believe the interpolation threshold occurs around 0.5M parameters, but most of our model capacities fall above this threshold.<br><br>

            For CIFAR-100, the results do not support our hypothesis that links overparameterization, sharpness, and generalization. We believe this is because we have not yet reached the interpolation threshold. While the training error decreases each time model capacity is increased, the generalization gap only increases. We believe the interpolation threshold is near 1.2M parameters as that is the first time the training error approaches 0%.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -50%);">
						The double descent phenomenon describes how test error can decrease again after initially increasing with model complexity.
		    </div>
		</div>

		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>5. Conclusion</h1>
            This project performed a systematic, quantitative geometric analysis of the loss landscape for ResNet architectures of varying depth, helping bridge the gap between theoretical properties of the optimization landscape and empirical model performance. Our findings support many core tenets of existing literature on mode connectivity, provide new information about the geometric properties of the connecting low-loss paths, and highlight novel, counter-intuitive insights that warrant further research.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>5.1 Key Findings</h2>
            Our analysis of the Bezier-optimized low-loss paths revealed a clear evolution of the loss landscape with increased network depth:<br><br>

            <b>Mode Connectivity and Barrier Height:</b> We confirmed that increased network depth reduces the barrier height along the curved paths between distinct solutions. Test accuracy loss along the path dropped from over 7% for the shallowest model (ResNet-8) to ~2% for the deepest models, indicating that overparameterization creates a smoother, more easily traversable loss manifold.<br><br>

            <b>Sharpness and Robustness:</b> We verified the hypothesis that deeper networks tend to converge to flatter minima, which literature associates with improved generalization. We demonstrated that sharpness is not uniform along the low-loss path, varying by a factor of 2 even when the loss and barrier height remain minimal.<br><br>

            <b>Path Curvature:</b> Contrary to our initial hypothesis, the low-loss paths connecting minima in deeper networks were found to be more strongly curved. This suggests that while overparameterization creates broad, connected basins, the optimizer must exploit higher-order non-linearities in parameter space to navigate the lowest-loss routes between them.<br><br>

            <b>Generalization Gap:</b> The inconsistency in the generalization gap results between CIFAR-10 and CIFAR-100 suggests that decreased sharpness alone is insufficient to guarantee improved generalization, and that the benefits may only manifest past the interpolation threshold. This requires further investigation with larger models.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>5.2 Significance</h2>
            The primary significance of this work lies in its quantitative findings, which move beyond the mere existence of low-loss paths to characterize their geometry.<br><br>

            <b>Theoretical Advancement:</b> The observed variation in sharpness along the path has profound implications for ensemble methods. It shows that traversing the loss manifold yields solutions that are not functionally equivalent in terms of robustness, necessitating that ensemble generation strategies incorporate sharpness as a selection criterion, not just low loss.<br><br>

            <b>Principled Model Design:</b> By establishing a clear, quantitative link between network depth and geometric metrics (barrier height, sharpness), our results enable more principled model design. Researchers can now anticipate the topological characteristics of the loss landscape induced by architectural choices and make more intelligent decisions on model architecture and optimizer selection.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>5.3 Future Work</h2>
            Given the fundamental nature of our findings and the computational constraints of this study, several avenues for future research are warranted:<br><br>

            <b>Architectural Universality Testing:</b> It would be valuable to apply these techniques to more complex and fundamentally different architectures, such as Transformers. The primary objective is to investigate if the observed geometric phenomena are universal properties of deep neural networks or if they are tied to specific model types, and to explore if modern complex architectures, where theory is harder to apply <a href="#ref_19">[19]</a>, present new phenomena.<br><br>

            <b>Efficiency of Path Discovery:</b> A deeper exploration is needed into the conditions under which the low-loss path transitions to being linear or near-linear. A computationally powerful result would involve predicting the existence and geometry of low-loss paths in complex architectures, similar to recent work on predicting optimizer dynamics <a href="#ref_20">[20]</a>, which could lead to novel insights into the loss landscape itself.<br><br>

            <b>Scaling to the Interpolating Regime:</b> Future work should extend the CIFAR-100 analysis into the "modern interpolating regime". This is necessary to fully characterize the generalization gap inconsistency and verify whether the benefits of increased capacity only manifest past the interpolation threshold.<br><br>

            <b>Optimizer Influence on Path Geometry:</b> The counter-intuitive observation of increasing path curvature warrants investigation into how different optimizer regularization techniques and hyperparameter settings might affect the non-linearity of the low-loss paths, aiming to identify methods that promote straighter, lower-curvature trajectories.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="references">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references"><br>
							
							<h1>References</h1>
							<a id="ref_1"></a>[1] T. Garipov, P. Izmailov, D. Podoprikhin, D. P. Vetrov, and A. G. Wilson, "<a href="https://arxiv.org/abs/1802.10026" target="_blank">Loss surfaces, mode connectivity, and fast ensembling of DNNs</a>", <i>Advances in Neural Information Processing Systems</i>, 2018.<br><br>
							<a id="ref_2"></a>[2] F. Draxler, K. Veschgini, M. Salmhofer, and F. Hamprecht, "<a href="https://arxiv.org/abs/1803.00885" target="_blank">Essentially no barriers in neural network energy landscape</a>", <i>Proceedings of the 35th International Conference on Machine Learning</i>, 2018.<br><br>
							<a id="ref_3"></a>[3] S. Sagawa, A. Raghunathan, P. W. Koh, and P. Liang, "<a href="https://arxiv.org/abs/2005.04345" target="_blank">An investigation of why overparameterization exacerbates spurious correlations</a>", <i>Proceedings of the 37th International Conference on Machine Learning</i>, 2020.<br><br>
							<a id="ref_4"></a>[4] H. Hassani and A. Javanmard, "<a href="https://arxiv.org/abs/2201.05149" target="_blank">The curse of overparametrization in adversarial training: Precise analysis of robust generalization for random features regression</a>", <i>The Annals of Statistics</i>, vol. 52, no. 2, pp. 441-465, 2024.<br><br>
							<a id="ref_5"></a>[5] Z. Zhu, F. Liu, G. Chrysos, and V. Cevher, "<a href="https://arxiv.org/abs/2209.07263" target="_blank">Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)</a>", <i>Advances in Neural Information Processing Systems</i>, 2022.<br><br>
							<a id="ref_6"></a>[6] R. Sun, D. Li, S. Liang, T. Ding, and R. Srikant, "<a href="https://arxiv.org/abs/2007.01429" target="_blank">The global landscape of neural networks: An overview</a>", <i>IEEE Signal Processing Magazine</i>, vol. 37, no. 5, pp. 95-108, 2020.<br><br>
							<a id="ref_7"></a>[7] Q. Nguyen, P. Bréchet, and M. Mondelli, "<a href="https://arxiv.org/abs/2102.09671" target="_blank">When are solutions connected in deep networks?</a>", <i>Proceedings of the 35th International Conference on Neural Information Processing Systems</i>, 2021.<br><br>
							<a id="ref_8"></a>[8] Y. Cooper, "<a href="https://epubs.siam.org/doi/10.1137/19M1308943" target="_blank">Global minima of overparameterized neural networks</a>", <i>SIAM Journal on Mathematics of Data Science</i>, vol. 3, no. 2, pp. 676-691, 2021.<br><br>
							<a id="ref_9"></a>[9] B. Simsek, F. Ged, A. Jacot, F. Spadaro, C. Hongler, W. Gerstner, and J. Brea, "<a href="https://arxiv.org/abs/2105.12221" target="_blank">Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances</a>", <i>Proceedings of the 38th International Conference on Machine Learning</i>, 2021.<br><br>
							<a id="ref_10"></a>[10] S. Jastrzebski, Z. Kenton, D. Arpit, N. Ballas, A. Fischer, A. Storkey, and Y. Bengio, "<a href="https://arxiv.org/abs/1711.04623" target="_blank">Three factors influencing minima in SGD</a>", 2018.<br><br>
							<a id="ref_11"></a>[11] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, "<a href="https://arxiv.org/abs/2010.01412" target="_blank">Sharpness-aware minimization for efficiently improving generalization</a>", <i>International Conference on Learning Representations</i>, 2021.<br><br>
							<a id="ref_12"></a>[12] P. Izmailov, D. Podoprikhin, T. Garipov, D. P. Vetrov, and A. G. Wilson, "<a href="https://arxiv.org/abs/1803.05407" target="_blank">Averaging weights leads to wider optima and better generalization</a>", <i>Conference on Uncertainty in Artificial Intelligence</i>, 2018.<br><br>
							<a id="ref_13"></a>[13] J. Kaddour, L. Liu, R. Silva, and M. J. Kusner, "<a href="https://openreview.net/forum?id=vDeh2yxTvuh" target="_blank">When do flat minima optimizers work?</a>", <i>Proceedings of the 36th International Conference on Neural Information Processing Systems</i>, 2022.<br><br>
							<a id="ref_14"></a>[14] D. Caldarola, B. Caputo, and M. Ciccone, "<a href="https://arxiv.org/abs/2203.11834" target="_blank">Improving generalization in federated learning by seeking flat minima</a>", <i>Computer Vision – ECCV 2022: 17th European Conference</i>, 2022.<br><br>
							<a id="ref_15"></a>[15] K. He, X. Zhang, S. Ren, and J. Sun, "<a href="https://arxiv.org/abs/1512.03385" target="_blank">Deep residual learning for image recognition</a>", <i>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, pp. 770-778, 2016.<br><br>
							<a id="ref_16"></a>[16] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, "<a href="https://arxiv.org/abs/1712.09913" target="_blank">Visualizing the loss landscape of neural nets</a>", <i>Proceedings of the 32nd International Conference on Neural Information Processing Systems</i>, 2018.<br><br>
							<a id="ref_17"></a>[17] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang, "<a href="https://arxiv.org/abs/1609.04836" target="_blank">On large-batch training for deep learning: Generalization gap and sharp minima</a>", <i>CoRR</i>, 2016.<br><br>
							<a id="ref_18"></a>[18] J. Martens and I. Sutskever, "<a href="https://www.cs.toronto.edu/~jmartens/docs/HF_book_chapter.pdf" target="_blank">Training Deep and Recurrent Networks with Hessian-Free Optimization</a>", pp. 479-535, Springer Berlin Heidelberg, 2012.<br><br>
							<a id="ref_19"></a>[19] L. Oneto, S. Ridella, and D. Anguita, "<a href="https://www.sciencedirect.com/science/article/pii/S0925231223003508" target="_blank">Do we really need a new theory to understand over-parameterization?</a>", <i>Neurocomputing</i>, vol. 543, p. 126227, 2023.<br><br>
							<a id="ref_20"></a>[20] J. Cohen, A. Damian, A. Talwalkar, J. Z. Kolter, and J. D. Lee, "<a href="https://arxiv.org/abs/2410.24206" target="_blank">Understanding optimization in deep learning with central flows</a>", <i>The Thirteenth International Conference on Learning Representations</i>, 2025.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>